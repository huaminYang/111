import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from torch import Tensor
import torch.nn.init as init

class Mish(nn.Module):
    def __init__(self):
        super().__init__()
        # print("Mish activation loaded...")
    def forward(self,x):
        x = x * (torch.tanh(F.softplus(x)))
        return x
class h_sigmoid(nn.Module):
    def __init__(self, inplace=True):
        super(h_sigmoid, self).__init__()
        self.relu = nn.ReLU6(inplace=inplace)

    def forward(self, x):
        return self.relu(x + 3) / 6
class GELU(nn.Module):
    def __init__(self):
        super(GELU, self).__init__()

    def forward(self, x):
        return 0.5*x*(1+torch.tanh(np.sqrt(2/np.pi)*(x+0.044715*torch.pow(x,3))))

class Conv2d_SN(nn.Module):
    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,
                 groups=1, bn_weight_init=1, bias=False,
                 norm_cfg=dict(type='BN', requires_grad=True)):
        super(Conv2d_SN, self).__init__()
        # super().__init__()
        self.inp_channel = a
        self.out_channel = b
        self.ks = ks
        self.pad = pad
        self.stride = stride
        self.dilation = dilation
        self.groups = groups
        # self.bias = bias
        self.conv = nn.Conv2d(a, b, ks, stride, pad, dilation, groups,bias=bias)
        self.bn = nn.BatchNorm2d(b)
        self.sn = SwitchNorm2d(b)
        # self.add_module('c', nn.Conv2d(
        #     a, b, ks, stride, pad, dilation, groups, bias=bias))
        # bn = build_norm_layer(norm_cfg, b)[1]
        # nn.init.constant_(bn.weight, bn_weight_init)
        # nn.init.constant_(bn.bias, 0)
        # self.add_module('bn', bn)
        self. init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                n //= m.groups
                m.weight.data.normal_(0, math.sqrt(2. / n))
                if m.bias is not None:
                    m.bias.data.zero_()
            # elif isinstance(m, nn.BatchNorm2d):
            #     m.weight.data.fill_(1)
            #     m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0, 0.01)
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x):
        x = self.conv(x)
        # x = self.bn(x)
        x = self.sn(x)
        return x


class SwitchNorm2d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.9, using_moving_average=True, using_bn=True,
                 last_gamma=False):
        super(SwitchNorm2d, self).__init__()
        self.eps = eps
        self.momentum = momentum
        self.using_moving_average = using_moving_average
        self.using_bn = using_bn
        self.last_gamma = last_gamma
        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))
        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))
        if self.using_bn:
            self.mean_weight = nn.Parameter(torch.ones(3))
            self.var_weight = nn.Parameter(torch.ones(3))
        else:
            self.mean_weight = nn.Parameter(torch.ones(2))
            self.var_weight = nn.Parameter(torch.ones(2))
        if self.using_bn:
            self.register_buffer('running_mean', torch.zeros(1, num_features, 1))
            self.register_buffer('running_var', torch.zeros(1, num_features, 1))

        self.reset_parameters()

    def reset_parameters(self):
        if self.using_bn:
            self.running_mean.zero_()
            self.running_var.zero_()
        if self.last_gamma:
            self.weight.data.fill_(0)
        else:
            self.weight.data.fill_(1)
        self.bias.data.zero_()

    def _check_input_dim(self, input):
        if input.dim() != 4:
            raise ValueError('expected 4D input (got {}D input)'
                             .format(input.dim()))

    def forward(self, x):
        self._check_input_dim(x)
        N, C, H, W = x.size()
        x = x.view(N, C, -1)
        mean_in = x.mean(-1, keepdim=True)
        var_in = x.var(-1, keepdim=True)

        mean_ln = mean_in.mean(1, keepdim=True)
        temp = var_in + mean_in ** 2
        var_ln = temp.mean(1, keepdim=True) - mean_ln ** 2

        if self.using_bn:
            if self.training:
                mean_bn = mean_in.mean(0, keepdim=True)
                var_bn = temp.mean(0, keepdim=True) - mean_bn ** 2
                if self.using_moving_average:
                    self.running_mean.mul_(self.momentum)
                    self.running_mean.add_((1 - self.momentum) * mean_bn.data)
                    self.running_var.mul_(self.momentum)
                    self.running_var.add_((1 - self.momentum) * var_bn.data)
                else:
                    self.running_mean.add_(mean_bn.data)
                    self.running_var.add_(mean_bn.data ** 2 + var_bn.data)
            else:
                mean_bn = torch.autograd.Variable(self.running_mean)
                var_bn = torch.autograd.Variable(self.running_var)

        softmax = nn.Softmax(0)
        mean_weight = softmax(self.mean_weight)
        var_weight = softmax(self.var_weight)

        if self.using_bn:
            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn
            var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn
        else:
            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln
            var = var_weight[0] * var_in + var_weight[1] * var_ln

        x = (x-mean) / (var+self.eps).sqrt()
        x = x.view(N, C, H, W)
        return x * self.weight + self.bias
class ccm_layer(nn.Module):
    def __init__(self,pixels_channels, k_size=[9, 17, 33, 65]):
        super(ccm_layer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)

        self.conv0 = nn.Conv1d(pixels_channels, pixels_channels, kernel_size=k_size[0], padding=(k_size[0] - 1) // 2)
        self.conv1 = nn.Conv1d(pixels_channels, pixels_channels, kernel_size=k_size[1], padding=(k_size[1] - 1) // 2)
        self.conv2 = nn.Conv1d(pixels_channels, pixels_channels, kernel_size=k_size[2], padding=(k_size[2] - 1) // 2)
        self.conv3 = nn.Conv1d(pixels_channels, pixels_channels, kernel_size=k_size[3], padding=(k_size[3] - 1) // 2)
        self.sigmoid = nn.Sigmoid()
        self.line = nn.Linear(4, 1)

    def forward(self, x):

        y = self.avg_pool(x).squeeze(-1)  #b,c,1
        # y = torch.transpose(self.avg_pool(x).squeeze(-1), dim0= 1,dim1=0)
        y0 = self.conv0(y).unsqueeze(-1)  #b,c,1,1
        y1 = self.conv1(y).unsqueeze(-1)
        y3 = self.conv2(y).unsqueeze(-1)
        y4 = self.conv3(y).unsqueeze(-1)
        y_full = self.line(torch.concat([y0, y1, y3, y4], axis=2).squeeze(-1)).unsqueeze(-1) #b,c,1,1
        y = self.sigmoid(y_full)

        x_fea = x * y.expand_as(x)

        return x_fea

class SPBlock(nn.Module):
    def __init__(self, inplanes, outplanes):
        super(SPBlock, self).__init__()
        midplanes = 2 * inplanes
        self.conv1 = nn.Conv2d(inplanes, midplanes, kernel_size=(3, 1), padding=(1, 0), bias=False)
        self.sn1 = SwitchNorm2d(midplanes)
        self.conv2 = nn.Conv2d(inplanes, midplanes, kernel_size=(1, 3), padding=(0, 1), bias=False)
        self.sn2 = SwitchNorm2d(midplanes)

        self.conv = nn.Conv2d(inplanes, midplanes,1)

        self.conv3 = nn.Conv2d(midplanes, outplanes, kernel_size=1, bias=True)
        self.pool1 = nn.AdaptiveAvgPool2d((None, 1))
        self.pool2 = nn.AdaptiveAvgPool2d((1, None))
        self.relu = nn.ReLU(inplace=False)
        self.sigmoid = h_sigmoid()

    def forward(self, x):
        _, _, h, w = x.size()  #xc=32,

        identity = self.conv(x)

        x1 = self.pool1(x)  #8,16,26,1
        x1 = self.conv1(x1) #8,32,26,1
        x1 = self.sn1(x1)
        # x1 = x1.expand(-1, -1, h, w) ##8,32,26,32
        #x1 = F.interpolate(x1, (h, w))

        x2 = self.pool2(x)  #1,63
        x2 = self.conv2(x2)
        x2 = self.sn2(x2)
        # x2 = x2.expand(-1, -1, h, w)
        #x2 = F.interpolate(x2, (h, w))

        x = torch.matmul(x1,x2)
        x = self.sigmoid(x)

        x = x * identity

        x = self.conv3(x)
        x = self.relu(x)

        return x

class CCM_strippool(torch.nn.Module):
    def __init__(self, dim, key_dim,
                 activation=None,
                 norm_cfg=dict(type='BN', requires_grad=True), ):
        super().__init__()
        self.scale = key_dim ** -0.5
        self.key_dim = key_dim
        self.sn = SwitchNorm2d(dim)
        self.sp = SPBlock(dim,key_dim)
        self.conv = nn.Conv2d(key_dim,dim,1)

        self.ccm = ccm_layer(dim, k_size=[3, 5, 7, 9]) #[3, 5, 7, 9]
        self.dwconv = Conv2d_SN(dim, key_dim, ks=3, stride=1, pad=1, dilation=1,
                                groups=key_dim, norm_cfg=norm_cfg)
        self.act = nn.ReLU6()
        self.pwconv = Conv2d_SN(key_dim, dim, ks=1, norm_cfg=norm_cfg)
        self.sigmoid = h_sigmoid()

    def forward(self, x):
        # B, C, H, W = x.shape c=32

        x = self.sn(x)

        # channel attention
        x_ccm = self.ccm(x)  #32

        # detail enhance
        # qkv = torch.cat([q, k, v], dim=1)   #16,256,26,31
        x1 = self.act(self.dwconv(x))
        qkv = self.pwconv(x1)    #16,32,26,31

        xx = self.sp(x_ccm)   #32
        xx = self.conv(xx)

        xx = xx * qkv

        xx = xx + x    #最后的残差 --之前没加

        return xx

class SSC8Block(nn.Module):

    #Split-and-Squeeze block  min_C = Cin/8
    def __init__(self,inplanes,planes):
        super(SSC8Block, self).__init__()

        self.conv = nn.Conv2d(inplanes,planes,1)
        self.conv_1 = nn.Conv2d(planes, planes//8, 1)
        self.conv_2 = nn.Conv2d(planes, inplanes, 1)

        # self.dwconv1 = nn.Conv2d(planes, planes//4, 3, 1, 1, groups=planes//4)
        # self.dwconv2 = nn.Conv2d(planes, planes//4, 3, 1, 1, groups=planes//4)
        # self.dwconv3 = nn.Conv2d(planes, planes//4, 3, 1, 1,  groups=planes//4)
        # self.dwconv4 = nn.Conv2d(planes, planes // 4, 3, 1, 1, groups=planes // 4)

        self.dwconv5 = nn.Conv2d(planes// 4, planes // 8, 1, 1, groups=planes // 8)
        self.dwconv6 = nn.Conv2d(planes// 4, planes // 8, 1, 1,groups=planes // 8)
        self.dwconv7 = nn.Conv2d(planes// 4, planes // 8, 1, 1, groups=planes // 8)
        self.dwconv8 = nn.Conv2d(planes// 4, planes // 8, 1, 1, groups=planes // 8)

        self.dwconv56 = nn.Conv2d(planes//4, planes//8, 3, 1, 1, groups=planes//8)
        self.dwconv78 = nn.Conv2d(planes//4, planes//8, 3, 1, 1, groups=planes//8)

        self.dwconv68 = nn.Conv2d(planes // 4, planes // 8, 3, 1, 1, groups=planes//8)

        self.sn = SwitchNorm2d(inplanes)
        self.split = planes // 4

        self.sn1 = SwitchNorm2d(planes)
        self.mish = Mish()

    def forward(self,x):

        x = self.sn(x)  #b,32
        x = self.conv(x)  #b,64

        x_1 = self.conv_1(x)

        x1, x2, x3, x4 = torch.split(x, self.split, dim=1)
        x5 = self.dwconv5(x1)  # b,16
        x6 = self.dwconv6(x2)
        x7 = self.dwconv7(x3)
        x8 = self.dwconv8(x4)

        x56 = torch.cat((x5,x6),dim=1)  #b,32
        x56 = self.dwconv56(x56)  #b,16

        x78 = torch.cat((x7,x8),dim=1)
        x78 = self.dwconv78(x78)  #16

        x68 = torch.cat((x56,x78),dim=1) #32
        x68 = self.dwconv68(x68)  #16

        x = torch.cat((x5,x6,x7,x8,x_1,x56,x78,x68),dim=1)

        x = self.sn1(x)
        x = self.mish(x)

        x = self.conv_2(x)

        return x

class Dual2SSCP_3SSC_up_2out(nn.Module):
    '''
         上支路2个sscp，上支路加fc,输出2个
       '''
    def __init__(self, height=3, width=3, shape=(26, 63), **kwargs):
        super(Dual2SSCP_3SSC_up_2out, self).__init__()
        self.height = height
        self.width = width
        # self.swap_index = swap_index

        self.conv1a = nn.Conv2d(kernel_size=(3, 1), in_channels=1, out_channels=16, padding=(1, 0))  # height padding
        self.conv1b = nn.Conv2d(kernel_size=(1, 3), in_channels=1, out_channels=16, padding=(0, 1))  # width padding

        self.conv_1 = nn.Conv2d(kernel_size=(1, 1), in_channels=1, out_channels=32)

        self.conv1 = nn.Conv2d(kernel_size=(1, 1), in_channels=32, out_channels=64)
        self.conv11 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=64)

        self.conv3 = nn.Conv2d(kernel_size=(3, 3), in_channels=16, out_channels=32, padding=1)
        self.gelu = GELU()

        self.conv1_16 = nn.Conv2d(kernel_size=(1, 1), in_channels=16, out_channels=32)
        self.conv1_32 = nn.Conv2d(kernel_size=(1, 1), in_channels=32, out_channels=64)
        self.conv1_48 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=128)

        self.conv64_32 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=32)
        # self.conv1_48 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=128)

        self.conv3_64 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=64, padding=1)

        self.ccsp1 = CCM_strippool(dim=32, key_dim=16)
        self.ccsp2 = CCM_strippool(dim=64, key_dim=32)

        self.ssc1 = SSC8Block(32, 64)
        self.ssc2 = SSC8Block(64, 128)
        self.ssc3 = SSC8Block(128, 256)

        self.convsn = Conv2d_SN(32, 32, 1)
        self.convsn2 = Conv2d_SN(64, 64, 1)
        self.convsn3 = Conv2d_SN(128, 128, 1)

        self.conv5 = nn.Conv2d(kernel_size=(5, 5), in_channels=128, out_channels=64, padding=2)
        # self.odconv = ODConv2d(kernel_size=5, in_planes=128, out_planes=64, kernel_num=1, padding=2)

        self.sigmoid = h_sigmoid()

        self.maxp = nn.MaxPool2d(kernel_size=(2, 2))
        self.avgp = nn.AvgPool2d((2, 2))

        self.drop = nn.Dropout(0.2)
        self.drop1 = nn.Dropout(0.5)

        self.sn2 = SwitchNorm2d(64)
        self.sn3 = SwitchNorm2d(64)

        self.bn5 = nn.BatchNorm2d(128)
        self.sn5 = SwitchNorm2d(64)

        dim = (shape[0] // 2) * (shape[1] // 4)  # 195

        i1 = 128 * dim  # 24960
        i = 64 * dim

        self.fc = nn.Linear(in_features=i, out_features=4)
        self.fc1 = nn.Linear(in_features=i1, out_features=4)

    def forward(self, x, *input):

        xa = self.conv1a(x)  # (32,16,26,63)·
        xa = self.gelu(xa)

        xb = self.conv1b(x)
        xb = self.gelu(xb)

        xab = torch.cat((xb, xa), 2)  # （32，16，52，63）

        xab = self.conv3(xab)  # (32,32,52,63)
        x = self.gelu(xab)

        x1 = self.ccsp1(x)  # (32,32,52,63)  64
        x1 = self.maxp(x1)  # (32,32,26,32)
        x1 = self.sn2(self.conv1(x1))  # b,64,26,32
        # x11 = self.sigmoid(x11)

        # x1 = self.conv1_32(x1)
        x1 = self.maxp(self.drop(self.ccsp2(x1)))  # (32,64,13,15)
        x11 = self.sn3(self.conv11(x1))  # c=64
        x11 = self.sigmoid(x11)

        x10 = self.conv64_32(x11)
        x1_1 = F.interpolate(x10, size=[52,63], mode='bilinear', align_corners=False)
        x = torch.mul(x1_1,x)

        x3 = self.ssc1(x)
        x3 = self.convsn(self.maxp(x3))  # b,32,

        x12 = F.interpolate(x10, size=[26, 31], mode='bilinear', align_corners=False)
        x_fuse1 = torch.mul(x12, x3)  # C= 32

        x3 = self.ssc2(self.conv1_32(x_fuse1))
        x3 = self.drop(x3)
        x3 = self.convsn2(self.maxp(x3))  #64
        #
        x_fuse2 = torch.mul(x11, x3)  #b,48,13,15

        x3= self.conv1_48(x_fuse2)  # (32,64, 12, 15) (32,128,13,15) 256
        x3 = self.ssc3(x3)  #128
        x3 = self.drop(x3)

        x3 = self.convsn3(x3)  #13,16

        x1 = self.conv1_48(x1)

        x = x1 + x3

        # x = self.conv3_64(x)
        x = self.conv5(x)  # (32,128,13,15) 64
        x = self.sn5(x)
        x_fea = F.relu(x)
        x = self.drop1(x_fea)


        # flatten 降维 --一维
        shape = x.shape  #
        shape1 = x1.shape  #

        # x = x.view(*x.shape[:-2], -1)  # （32，128，195）

        # x = x.contiguous().permute(0, 2, 3, 1).view(shape[0],shape[3] * shape[2], shape[1])  #(32,195,128)
        #
        x_features = x.reshape(x.shape[0], -1)  # (32,13312)
        x = self.fc(x_features)  # （32，4）

        x1 = x1.reshape(x1.shape[0], -1)  # (32,13312)
        x1 = self.fc1(x1)  # （32，4）

        return x1, x
        # return x

